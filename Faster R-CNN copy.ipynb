{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(example, feature_description)\n",
    "\n",
    "    # Decode image\n",
    "    image = tf.image.decode_jpeg(parsed_example['image/encoded'])\n",
    "    image = tf.image.resize(image, [512, 512]) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Decode bounding boxes and labels\n",
    "    xmin = tf.sparse.to_dense(parsed_example['image/object/bbox/xmin'])\n",
    "    xmax = tf.sparse.to_dense(parsed_example['image/object/bbox/xmax'])\n",
    "    ymin = tf.sparse.to_dense(parsed_example['image/object/bbox/ymin'])\n",
    "    ymax = tf.sparse.to_dense(parsed_example['image/object/bbox/ymax'])\n",
    "    labels = tf.sparse.to_dense(parsed_example['image/object/class/label'])\n",
    "\n",
    "    bboxes = tf.stack([ymin, xmin, ymax, xmax], axis=-1)  # Combine into (ymin, xmin, ymax, xmax) format\n",
    "    return image, bboxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_path, batch_size=8):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "    dataset = parsed_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False, input_shape=(512, 512, 3)\n",
    ")\n",
    "\n",
    "feature_extractor = models.Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model.get_layer(\"conv4_block6_out\").output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rpn(feature_maps):\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(feature_maps)\n",
    "    objectness_scores = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "    bbox_deltas = layers.Conv2D(4, (1, 1))(x)\n",
    "    return objectness_scores, bbox_deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_align(feature_maps, rois, output_size=(7, 7)):\n",
    "    \"\"\"\n",
    "    Perform ROI Align on given feature maps using provided ROIs.\n",
    "\n",
    "    Args:\n",
    "        feature_maps: Tensor with shape [batch_size, height, width, channels].\n",
    "        rois: A tensor of shape [num_rois, 4] representing normalized bounding boxes (in range [0, 1]).\n",
    "        output_size: Tuple specifying the desired cropped size (default is (7, 7)).\n",
    "\n",
    "    Returns:\n",
    "        roi_features: Cropped features after applying ROI Align.\n",
    "    \"\"\"\n",
    "    # Determine number of ROIs\n",
    "    num_rois = tf.shape(rois)[0]\n",
    "    \n",
    "    # Ensure `rois` has the correct rank (remove any unexpected extra dimensions)\n",
    "    rois = tf.reshape(rois, [num_rois, 4])  # Shape must be [num_rois, 4]\n",
    "\n",
    "    # Prepare the batch indices corresponding to each ROI\n",
    "    batch_size = tf.shape(feature_maps)[0]\n",
    "    batch_indices = tf.random.uniform(\n",
    "        [num_rois], minval=0, maxval=batch_size, dtype=tf.int32\n",
    "    )  # Random valid indices for batch\n",
    "\n",
    "    # Use crop_and_resize for ROI alignment\n",
    "    roi_features = tf.image.crop_and_resize(\n",
    "        feature_maps,  # Input feature map\n",
    "        boxes=rois,  # Normalized bounding boxes\n",
    "        box_indices=batch_indices,  # Indices indicating the batch dimension\n",
    "        crop_size=output_size  # Desired size for ROI (e.g., 7x7)\n",
    "    )\n",
    "    \n",
    "    return roi_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images, target_size=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Resize images to the expected input size of ResNet50.\n",
    "    Args:\n",
    "        images: Input images of shape (batch_size, h, w, 3).\n",
    "        target_size: Desired target size, default is (1024, 1024).\n",
    "\n",
    "    Returns:\n",
    "        Resized images with the shape matching the target_size.\n",
    "    \"\"\"\n",
    "    # Resize each image in the batch\n",
    "    resized_images = tf.image.resize(images, target_size)\n",
    "    return resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionHead(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A simple classification head for ROI features.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # Define the layers only once (weights will be initialized once)\n",
    "        self.dense1 = layers.Dense(256, activation='relu')\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "        self.bbox_regressor = layers.Dense(4, activation=None)\n",
    "\n",
    "    def call(self, roi_features):\n",
    "        \"\"\"\n",
    "        Forward pass through the detection head.\n",
    "        \n",
    "        Args:\n",
    "            roi_features: Input features of shape [num_rois, 7, 7, channels].\n",
    "\n",
    "        Returns:\n",
    "            class_scores: The predicted class scores.\n",
    "            bbox_deltas: The predicted bounding box deltas.\n",
    "        \"\"\"\n",
    "        # Flatten the spatial dimensions (7x7) to make it compatible with Dense layers\n",
    "        x = tf.keras.layers.Flatten()(roi_features)  # Flatten from [7,7,channels] to 1D\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        # Class scores and bounding box predictions\n",
    "        class_scores = self.classifier(x)  # Classification predictions\n",
    "        bbox_deltas = self.bbox_regressor(x)  # Bounding box offsets\n",
    "        \n",
    "        return class_scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_loss(objectness_pred, objectness_true, bbox_pred, bbox_true):\n",
    "    obj_loss = tf.keras.losses.BinaryCrossentropy()(objectness_true, objectness_pred)\n",
    "    bbox_loss = tf.keras.losses.Huber()(bbox_true, bbox_pred)\n",
    "    return obj_loss + bbox_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_loss(objectness_pred, objectness_true, bbox_pred, bbox_true):\n",
    "    obj_loss = tf.keras.losses.BinaryCrossentropy()(objectness_true, objectness_pred)\n",
    "    bbox_loss = tf.keras.losses.Huber()(bbox_true, bbox_pred)\n",
    "    return obj_loss + bbox_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI Features Shape: (10, 7, 7, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Example feature map tensor\n",
    "feature_maps = tf.random.normal([8, 32, 32, 1024])  # [batch_size=8, height=32, width=32, channels=1024]\n",
    "\n",
    "# Example normalized ROIs\n",
    "rois = tf.random.uniform([10, 4], minval=0.0, maxval=1.0)  # Generate 10 random ROIs normalized in [0,1]\n",
    "\n",
    "# Call roi_align\n",
    "roi_features = roi_align(feature_maps, rois)\n",
    "print(\"ROI Features Shape:\", roi_features.shape)  # Expected: [10, 7, 7, 1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backbone():\n",
    "    \"\"\"\n",
    "    Create the backbone feature extraction model using ResNet50 as an example.\n",
    "    Set input shape to match the actual data dimensions.\n",
    "    \"\"\"\n",
    "    # Use the actual image size (512, 512) as input\n",
    "    base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(1024, 1024, 3))\n",
    "\n",
    "    # Optionally freeze some layers for transfer learning\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  # Freeze the layers for transfer learning; set to True for fine-tuning\n",
    "    \n",
    "    return base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model globally\n",
    "def create_model():\n",
    "    # Example: A simple CNN model\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, input_shape=(512, 512, 3)\n",
    "    )\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    # Compile the model to ensure compatibility\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, bboxes, labels):\n",
    "    \"\"\"\n",
    "    A single training step.\n",
    "\n",
    "    Args:\n",
    "        images: Input images batch.\n",
    "        bboxes: Bounding boxes.\n",
    "        labels: Classification labels.\n",
    "\n",
    "    Returns:\n",
    "        Loss value for backpropagation.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Resize images if necessary\n",
    "        images_resized = resize_images(images)  # Resize images as shown before\n",
    "\n",
    "        # Generate feature maps from the backbone\n",
    "        feature_maps = backbone_model(images_resized)\n",
    "        rois = roi_align(feature_maps, bboxes)  # Extract ROIs\n",
    "        \n",
    "        # Pass ROIs through detection head\n",
    "        roi_features = tf.image.resize(rois, [7, 7])  # Resize for compatibility\n",
    "        class_scores, bbox_deltas = head(roi_features)  # Pass through DetectionHead\n",
    "        \n",
    "        # Calculate classification loss\n",
    "        classification_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, class_scores)\n",
    "        \n",
    "        # Calculate bounding box loss\n",
    "        bbox_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "        bbox_loss = bbox_loss_fn(bboxes, bbox_deltas)  # Mean absolute error computation\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = tf.reduce_mean(classification_loss + bbox_loss)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, head.trainable_variables + backbone_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, head.trainable_variables + backbone_model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset loaded successfully.\n",
      "Train dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "train_file = r\"C:\\Users\\ACER\\Desktop\\IS\\Obj_recognition_Version_1\\tfrecords\\train.record\"\n",
    "valid_file = r\"C:\\Users\\ACER\\Desktop\\IS\\Obj_recognition_Version_1\\tfrecords\\valid.record\"\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    raise FileNotFoundError(f\"File not found at {train_file}\")\n",
    "\n",
    "if not os.path.exists(valid_file):\n",
    "    raise FileNotFoundError(f\"File not found at {valid_file}\")\n",
    "\n",
    "train_dataset = load_tfrecord_dataset(train_file, batch_size=48)\n",
    "print(\"Train dataset loaded successfully.\")\n",
    "valid_dataset = load_tfrecord_dataset(valid_file, batch_size=48)\n",
    "print(\"Train dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (48, 512, 512, 3)\n",
      "Bounding boxes: tf.Tensor(\n",
      "[[[0.733      0.70266664 0.956      0.8333333 ]]\n",
      "\n",
      " [[0.364      0.35466668 0.542      0.41933334]]\n",
      "\n",
      " [[0.304      0.72866666 0.481      0.824     ]]\n",
      "\n",
      " [[0.325      0.78066665 0.864      0.91      ]]\n",
      "\n",
      " [[0.58       0.47733334 0.829      0.5653333 ]]\n",
      "\n",
      " [[0.30927834 0.44015443 0.47938144 0.58301157]]\n",
      "\n",
      " [[0.825      0.16066666 1.         0.53      ]]\n",
      "\n",
      " [[0.54       0.62133336 1.         0.8326667 ]]\n",
      "\n",
      " [[0.882      0.012      0.95       0.254     ]]\n",
      "\n",
      " [[0.354      0.34266666 0.515      0.424     ]]\n",
      "\n",
      " [[0.576      0.29733333 0.843      0.42466667]]\n",
      "\n",
      " [[0.195      0.706      0.309      0.78533334]]\n",
      "\n",
      " [[0.512      0.524      0.903      0.748     ]]\n",
      "\n",
      " [[0.40729168 0.4484375  0.48958334 0.478125  ]]\n",
      "\n",
      " [[0.48       0.49466667 0.52       0.546     ]]\n",
      "\n",
      " [[0.563      0.5553333  0.646      0.62866664]]\n",
      "\n",
      " [[0.536      0.17866667 0.767      0.25266665]]\n",
      "\n",
      " [[0.299      0.022      0.577      0.16333333]]\n",
      "\n",
      " [[0.595      0.32       0.822      0.43733335]]\n",
      "\n",
      " [[0.138      0.8093333  0.91       0.9946667 ]]\n",
      "\n",
      " [[0.414      0.236      0.548      0.33      ]]\n",
      "\n",
      " [[0.393      0.324      0.467      0.37666667]]\n",
      "\n",
      " [[0.472      0.49733335 0.761      0.61466664]]\n",
      "\n",
      " [[0.368      0.442      0.721      0.612     ]]\n",
      "\n",
      " [[0.458      0.8293333  0.573      0.8973333 ]]\n",
      "\n",
      " [[0.161      0.56266665 0.336      0.8066667 ]]\n",
      "\n",
      " [[0.12       0.41733333 0.745      0.56133336]]\n",
      "\n",
      " [[0.449      0.7546667  1.         0.994     ]]\n",
      "\n",
      " [[0.64       0.83066666 1.         1.        ]]\n",
      "\n",
      " [[0.         0.842      0.198      0.9066667 ]]\n",
      "\n",
      " [[0.288      0.         0.739      0.092     ]]\n",
      "\n",
      " [[0.55       0.404      0.851      0.6533333 ]]\n",
      "\n",
      " [[0.587      0.29333332 0.828      0.406     ]]\n",
      "\n",
      " [[0.46       0.65133333 0.936      0.97933334]]\n",
      "\n",
      " [[0.436      0.556      0.679      1.        ]]\n",
      "\n",
      " [[0.641      0.16866666 0.976      0.31133333]]\n",
      "\n",
      " [[0.452      0.53466666 0.711      0.63733333]]\n",
      "\n",
      " [[0.481      0.102      0.678      0.40333334]]\n",
      "\n",
      " [[0.706      0.538      1.         0.8013333 ]]\n",
      "\n",
      " [[0.671      0.35133332 0.943      0.746     ]]\n",
      "\n",
      " [[0.556      0.37466666 0.891      0.6473333 ]]\n",
      "\n",
      " [[0.514      0.072      0.64       0.14333333]]\n",
      "\n",
      " [[0.6        0.20333333 1.         0.81866664]]\n",
      "\n",
      " [[0.066      0.47066668 1.         0.716     ]]\n",
      "\n",
      " [[0.556      0.60466665 0.638      0.72466666]]\n",
      "\n",
      " [[0.71       0.40133333 1.         0.79066664]]\n",
      "\n",
      " [[0.592      0.40133333 1.         0.70066667]]\n",
      "\n",
      " [[0.261      0.44666666 0.489      0.64533335]]], shape=(48, 1, 4), dtype=float32)\n",
      "Labels: tf.Tensor(\n",
      "[[15]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [13]\n",
      " [ 7]\n",
      " [15]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 7]], shape=(48, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for images, bboxes, labels in train_dataset.take(1):\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Bounding boxes:\", bboxes)\n",
    "    print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = create_backbone()\n",
    "\n",
    "# Create detection head\n",
    "num_classes = 19\n",
    "head = DetectionHead(num_classes)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Step 0/209, Loss: 169.4749\n",
      "Step 10/209, Loss: 115.4429\n",
      "Step 20/209, Loss: 43.8944\n",
      "Step 30/209, Loss: 15.1025\n",
      "Step 40/209, Loss: 15.8117\n",
      "Step 50/209, Loss: 5.6910\n",
      "Step 60/209, Loss: 3.5114\n",
      "Step 70/209, Loss: 3.4824\n",
      "Step 80/209, Loss: 3.5026\n",
      "Step 90/209, Loss: 3.4589\n",
      "Step 100/209, Loss: 3.4352\n",
      "Step 110/209, Loss: 3.4245\n",
      "Step 120/209, Loss: 3.3749\n",
      "Step 130/209, Loss: 3.4145\n",
      "Step 140/209, Loss: 3.3783\n",
      "Step 150/209, Loss: 3.3931\n",
      "Step 160/209, Loss: 3.3462\n",
      "Step 170/209, Loss: 3.3554\n",
      "Step 180/209, Loss: 3.3351\n",
      "Step 190/209, Loss: 3.3501\n",
      "Step 200/209, Loss: 3.2730\n",
      "Epoch 1 Average Loss: 17.0975\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filename must end in `.weights.h5`. Received: filepath=Saved_weights\\epoch_1.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Save weights at the end of each epoch\u001b[39;00m\n\u001b[0;32m     31\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved weights to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:224\u001b[0m, in \u001b[0;36msave_weights\u001b[1;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.saving.save_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_weights\u001b[39m(model, filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 224\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filename must end in `.weights.h5`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m         )\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filepath)\n",
      "\u001b[1;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=Saved_weights\\epoch_1.h5"
     ]
    }
   ],
   "source": [
    "# Set your weights save directory\n",
    "weights_dir = \"Saved_weights\"\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 2\n",
    "batch_size= 48\n",
    "num_samples = sum(1 for _ in train_dataset.unbatch())\n",
    "steps_per_epoch = math.ceil(num_samples / batch_size)\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0  # Track average loss for the epoch\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for step, (images, bboxes, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        # Run the custom training step\n",
    "        loss = train_step(images, bboxes, labels)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        if step % 10 == 0:  # Log progress\n",
    "            print(f\"Step {step}/{steps_per_epoch}, Loss: {loss:.4f}\")\n",
    "            \n",
    "\n",
    "    # Average the epoch loss\n",
    "    epoch_loss /= steps_per_epoch\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save weights at the end of each epoch\n",
    "    weights_path = os.path.join(weights_dir, f\"epoch_{epoch + 1}.h5\")\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"Saved weights to: {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: Saved_model\\trained_model_updated.h5\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"Saved_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"trained_model_updated.h5\")\n",
    "model.save(model_path)\n",
    "print(f\"Saved model to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from: Saved_model/trained_model.h5\n",
      "Epoch 1/1\n",
      "Step 0/209, Loss: 3.3016\n",
      "Step 10/209, Loss: 3.2890\n",
      "Step 20/209, Loss: 3.2741\n",
      "Step 30/209, Loss: 3.2544\n",
      "Step 40/209, Loss: 3.2132\n",
      "Step 50/209, Loss: 3.2307\n",
      "Step 60/209, Loss: 3.2408\n",
      "Step 70/209, Loss: 3.2175\n",
      "Step 80/209, Loss: 3.1929\n",
      "Step 90/209, Loss: 3.1904\n",
      "Step 100/209, Loss: 3.1571\n",
      "Step 110/209, Loss: 3.1197\n",
      "Step 120/209, Loss: 3.1247\n",
      "Step 130/209, Loss: 3.1620\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Saved_model/trained_model.h5\"\n",
    "model = tf.keras.models.load_model(model_path, compile=False)  # Avoid issues with compiled metrics\n",
    "print(f\"Loaded model from: {model_path}\")\n",
    "\n",
    "# Recompile the model to ensure optimizer and metrics are set\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',  # Update this to your specific loss function\n",
    "    metrics=['accuracy']  # Use the metrics relevant to your task\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0  # Track average loss for the epoch\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for step, (images, bboxes, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        # Run the custom training step\n",
    "        loss = train_step(images, bboxes, labels)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        if step % 10 == 0:  # Log progress\n",
    "            print(f\"Step {step}/{steps_per_epoch}, Loss: {loss:.4f}\")\n",
    "            \n",
    "\n",
    "    # Average the epoch loss\n",
    "    epoch_loss /= steps_per_epoch\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    updated_model_path = \"Saved_model/updated_trained_model.h5\"\n",
    "    model.save(updated_model_path)\n",
    "    print(f\"Updated model saved to: {updated_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset):\n",
    "    for images, bboxes, labels in dataset:\n",
    "        class_scores, bbox_deltas = predict(images)  # Replace with the prediction function\n",
    "        # Add evaluation logic such as IoU calculation or mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    feature_maps = feature_extractor(image)\n",
    "    rpn_scores, rpn_deltas = build_rpn(feature_maps)\n",
    "    rois = tf.random.uniform((1, 10, 4), 0, 1)  # Replace with actual proposals\n",
    "    roi_features = roi_align(feature_maps, rois, (7, 7))\n",
    "    class_scores, bbox_deltas = build_head(roi_features, num_classes=2)\n",
    "    return class_scores, bbox_deltas\n",
    "\n",
    "test_image = next(iter(valid_dataset))[0]  # Example test image\n",
    "class_scores, bbox_deltas = predict(test_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
